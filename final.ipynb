{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AFZhPkPKrDAyugspTyYB2X3tjnU6vVCq",
      "authorship_tag": "ABX9TyNS7JoNDbXS5uMuYW8KBV/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faridlyk/music-genre-classifier-cnn-dnn/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias"
      ],
      "metadata": {
        "id": "xYYYigjp1ugW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5Sk69-fG76BX",
        "outputId": "3d041879-4d86-4065-8ac5-346d83c2bfae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp\n",
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyyIjXIRPUAX",
        "outputId": "355fc3a4-d5fc-40cb-cb51-11bf6bf55d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.11.4-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2024.11.4-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2024.11.4\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from moviepy.editor import AudioFileClip\n",
        "import matplotlib.pyplot as plt\n",
        "import yt_dlp\n",
        "from pydub import AudioSegment\n",
        "import joblib"
      ],
      "metadata": {
        "id": "YOTVRTZ76Tpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d1d2dd-b1d3-47a6-8abf-8d10932c92a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Código no usado"
      ],
      "metadata": {
        "id": "6qIG1bjw1zTG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYHDZB9846yh"
      },
      "outputs": [],
      "source": [
        "# Extract audio features\n",
        "\n",
        "\"\"\"\n",
        "Extraction of audio features with the librosa library.\n",
        "NOT USED IN THE PROJECT.\n",
        "\"\"\"\n",
        "\n",
        "def extract_features(file_path):\n",
        "  try:\n",
        "    audio, sample_rate = librosa.load(file_path, duration=30)\n",
        "\n",
        "    features = {\n",
        "        'mfccs': np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40), axis=0),\n",
        "        'spectral_centroids': np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate)),\n",
        "        'chroma_stft': np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate)),\n",
        "        'melspectrogram': np.mean(librosa.feature.melspectrogram(y=audio, sr=sample_rate)),\n",
        "        'contrast': np.mean(librosa.feature.spectral_contrast(y=audio, sr=sample_rate)),\n",
        "        'rolloff': np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sample_rate)),\n",
        "        'zcr': np.mean(librosa.feature.zero_crossing_rate(y=audio)),\n",
        "        'tempo': np.mean(librosa.feature.rhythm.tempo(y=audio, sr=sample_rate))\n",
        "    }\n",
        "    mfccs_mean = np.mean(features['mfccs'])\n",
        "\n",
        "    features_array = np.array([\n",
        "        mfccs_mean,\n",
        "        features['spectral_centroids'],\n",
        "        features['chroma_stft'],\n",
        "        features['melspectrogram'],\n",
        "        features['contrast'],\n",
        "        features['rolloff'],\n",
        "        features['zcr'],\n",
        "        features['tempo']\n",
        "    ])\n",
        "\n",
        "    return features_array\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error extracting features from {file_path}: {str(e)}\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Processes each file within each folder.\n",
        "NOT USED IN THE PROJECT.\n",
        "\"\"\"\n",
        "\n",
        "def process_data(data_path):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # Process each genre folder\n",
        "    for genre in os.listdir(data_path):\n",
        "        genre_path = os.path.join(data_path, genre)\n",
        "        if os.path.isdir(genre_path):\n",
        "            print(f\"Processing {genre} files...\")\n",
        "\n",
        "            # Process each audio file in the genre folder\n",
        "            for file_name in os.listdir(genre_path):\n",
        "                if file_name.endswith('.wav'):\n",
        "                    file_path = os.path.join(genre_path, file_name)\n",
        "                    extracted_features = extract_features(file_path)\n",
        "\n",
        "                    if extracted_features is not None and len(extracted_features) > 0:\n",
        "                        features.append(extracted_features)\n",
        "                        labels.append(genreToNumber(genre))\n",
        "\n",
        "    print(\"Data processing completed.\")\n",
        "\n",
        "    return np.array(features), np.array(labels)"
      ],
      "metadata": {
        "id": "0U-UFyTiIPJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funciones"
      ],
      "metadata": {
        "id": "nABRJy6c2LpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def genreToNumber(genre):\n",
        "    genre_dict = {\n",
        "        'blues': 0,\n",
        "        'classical': 1,\n",
        "        'country': 2,\n",
        "        'disco': 3,\n",
        "        'hiphop': 4,\n",
        "        'jazz': 5,\n",
        "        'metal': 6,\n",
        "        'pop': 7,\n",
        "        'reggae': 8,\n",
        "        'rock': 9\n",
        "    }\n",
        "    return genre_dict.get(genre, 9)"
      ],
      "metadata": {
        "id": "6-uXvsreNyph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the features from the dataset file (59 features)\n",
        "def process_csv(file_path):\n",
        "  df = pd.read_csv(file_path)\n",
        "\n",
        "  # Archivo corrupto\n",
        "  df = df[df['filename'] != 'jazz.00054.wav']\n",
        "\n",
        "  # Changes gender names to numbers\n",
        "  labels = df['label'].apply(genreToNumber)\n",
        "\n",
        "  # features = df.filter(regex='mean$')\n",
        "  features = df.filter(regex='(mean$|var$)|(^tempo$)')\n",
        "  print(\"Data features processing completed.\\n\")\n",
        "\n",
        "  return features, labels\n",
        "\n",
        "# file_path = '/content/drive/MyDrive/datasets/Data/features_30_sec.csv'\n",
        "# features, labels = process_csv(file_path)\n",
        "\n",
        "# if features is not None and labels is not None:\n",
        "#     print(\"Features shape:\", features.shape)\n",
        "#     print(\"Labels shape:\", labels.shape)\n",
        "#     print('\\n')\n",
        "#     print(features)"
      ],
      "metadata": {
        "id": "FWSta1VqTyTb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-procesamiento de los espectrogramas de Mel\n",
        "def extract_img_features(img_path):\n",
        "\n",
        "  image = Image.open(img_path).convert('L')\n",
        "  image = image.resize((128, 128))\n",
        "  image_array = np.array(image)\n",
        "\n",
        "  # Normalizes data from 0..1\n",
        "  image_array = image_array / 255.0\n",
        "  image_array = np.expand_dims(image_array, axis=-1)\n",
        "\n",
        "  return image_array"
      ],
      "metadata": {
        "id": "vfQ1HCJxu3iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_img_data(img_path):\n",
        "    img_features = []\n",
        "\n",
        "    # Process each genre folder\n",
        "    for genre in os.listdir(img_path):\n",
        "        genre_path = os.path.join(img_path, genre)\n",
        "        if os.path.isdir(genre_path):\n",
        "            # print(f\"Processing {genre} images files...\")\n",
        "\n",
        "            # Process each image file in the genre folder\n",
        "            for img_name in os.listdir(genre_path):\n",
        "                if img_name.endswith('.png'):\n",
        "                    file_path = os.path.join(genre_path, img_name)\n",
        "                    extracted_img_features = extract_img_features(file_path)\n",
        "\n",
        "                    if extracted_img_features is not None and len(extracted_img_features) > 0:\n",
        "                        img_features.append(extracted_img_features)\n",
        "\n",
        "    print(\"Images processing completed.\\n\")\n",
        "\n",
        "    return np.array(img_features)"
      ],
      "metadata": {
        "id": "5dsJcNjEMXJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Código central"
      ],
      "metadata": {
        "id": "b55DsxPN2SEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  DATA_PATH = '/content/drive/MyDrive/datasets/Data/genres_original'\n",
        "  IMG_PATH = '/content/drive/MyDrive/datasets/mel_img'\n",
        "  FEATURES_PATH = '/content/drive/MyDrive/datasets/Data/features_30_sec.csv'\n",
        "\n",
        "  data_features, labels = process_csv(FEATURES_PATH)\n",
        "  img_features = process_img_data(IMG_PATH)\n",
        "\n",
        "  # print(\"Features shape:\", data_features.shape)\n",
        "  # print(\"Labels shape:\", labels.shape)\n",
        "  # print('Images shape:', img_features.shape)\n",
        "\n",
        "  X_train_features, X_val_features, X_train_images, X_val_images, y_train, y_val = train_test_split(\n",
        "      data_features, img_features, labels, test_size=0.2, random_state=17\n",
        "  )\n",
        "\n",
        "  feature_scaler = StandardScaler()\n",
        "  X_train_features = feature_scaler.fit_transform(X_train_features)\n",
        "  X_val_features = feature_scaler.transform(X_val_features)\n",
        "  joblib.dump(feature_scaler, '/content/drive/MyDrive/datasets/scalers/feature_scaler.pkl')\n",
        "\n",
        "  input_features = layers.Input(shape=(57,))\n",
        "  input_images = layers.Input(shape=(128, 128, 1))\n",
        "\n",
        "  # Dense layer for features\n",
        "  x1 = layers.Dense(64, activation='relu')(input_features)\n",
        "  x1 = layers.Dense(32, activation='relu')(x1)\n",
        "\n",
        "  # Convolutional layer for images\n",
        "  x2 = layers.Conv2D(32, (3, 3), activation='relu')(input_images)\n",
        "  x2 = layers.MaxPooling2D((2, 2))(x2)\n",
        "  x2 = layers.Conv2D(64, (3, 3), activation='relu')(x2)\n",
        "  x2 = layers.MaxPooling2D((2, 2))(x2)\n",
        "  x2 = layers.Flatten()(x2)\n",
        "\n",
        "  combined = layers.concatenate([x1, x2])\n",
        "\n",
        "  # Final dense layer\n",
        "  x = layers.Dense(64, activation='relu')(combined)\n",
        "  output = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "  # Create the model\n",
        "  model = keras.Model(inputs=[input_features, input_images], outputs=output)\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # Training the model\n",
        "  history = model.fit(\n",
        "      [X_train_features, X_train_images],\n",
        "      y_train,\n",
        "      validation_data=([X_val_features, X_val_images], y_val),\n",
        "      epochs=60,\n",
        "      batch_size=32\n",
        "  )\n",
        "\n",
        "  # Model evaluation\n",
        "  loss, accuracy = model.evaluate([X_val_features, X_val_images], y_val)\n",
        "  print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "  model.save('/content/drive/MyDrive/datasets/models/modelv3.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_r1jYpW6LJK",
        "outputId": "3f508059-6484-45c7-d564-a229a9f2be65",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data features processing completed.\n",
            "\n",
            "Images processing completed.\n",
            "\n",
            "Epoch 1/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 964ms/step - accuracy: 0.1208 - loss: 2.4501 - val_accuracy: 0.2300 - val_loss: 2.0993\n",
            "Epoch 2/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 775ms/step - accuracy: 0.3557 - loss: 1.9683 - val_accuracy: 0.4300 - val_loss: 1.6610\n",
            "Epoch 3/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 782ms/step - accuracy: 0.5062 - loss: 1.4266 - val_accuracy: 0.5000 - val_loss: 1.2776\n",
            "Epoch 4/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 774ms/step - accuracy: 0.5869 - loss: 1.1483 - val_accuracy: 0.5600 - val_loss: 1.1694\n",
            "Epoch 5/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 812ms/step - accuracy: 0.7183 - loss: 0.8743 - val_accuracy: 0.6550 - val_loss: 1.0334\n",
            "Epoch 6/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 867ms/step - accuracy: 0.7240 - loss: 0.7417 - val_accuracy: 0.6000 - val_loss: 1.0942\n",
            "Epoch 7/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 823ms/step - accuracy: 0.7912 - loss: 0.6119 - val_accuracy: 0.6050 - val_loss: 1.0811\n",
            "Epoch 8/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 939ms/step - accuracy: 0.8427 - loss: 0.4828 - val_accuracy: 0.7050 - val_loss: 0.8406\n",
            "Epoch 9/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 879ms/step - accuracy: 0.8943 - loss: 0.3628 - val_accuracy: 0.7250 - val_loss: 0.8039\n",
            "Epoch 10/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 767ms/step - accuracy: 0.9190 - loss: 0.3200 - val_accuracy: 0.7300 - val_loss: 0.9112\n",
            "Epoch 11/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 830ms/step - accuracy: 0.9443 - loss: 0.2322 - val_accuracy: 0.6700 - val_loss: 0.9585\n",
            "Epoch 12/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 834ms/step - accuracy: 0.9652 - loss: 0.1584 - val_accuracy: 0.7350 - val_loss: 0.8250\n",
            "Epoch 13/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 777ms/step - accuracy: 0.9734 - loss: 0.1290 - val_accuracy: 0.7200 - val_loss: 0.8610\n",
            "Epoch 14/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 900ms/step - accuracy: 0.9898 - loss: 0.0830 - val_accuracy: 0.7600 - val_loss: 0.8507\n",
            "Epoch 15/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 782ms/step - accuracy: 0.9923 - loss: 0.0692 - val_accuracy: 0.7500 - val_loss: 0.7855\n",
            "Epoch 16/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 786ms/step - accuracy: 0.9967 - loss: 0.0466 - val_accuracy: 0.7450 - val_loss: 0.8391\n",
            "Epoch 17/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 931ms/step - accuracy: 0.9988 - loss: 0.0321 - val_accuracy: 0.7600 - val_loss: 0.7901\n",
            "Epoch 18/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 785ms/step - accuracy: 1.0000 - loss: 0.0248 - val_accuracy: 0.7650 - val_loss: 0.8075\n",
            "Epoch 19/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 787ms/step - accuracy: 1.0000 - loss: 0.0171 - val_accuracy: 0.7800 - val_loss: 0.7989\n",
            "Epoch 20/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 867ms/step - accuracy: 1.0000 - loss: 0.0151 - val_accuracy: 0.7700 - val_loss: 0.8294\n",
            "Epoch 21/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 789ms/step - accuracy: 1.0000 - loss: 0.0116 - val_accuracy: 0.7950 - val_loss: 0.8628\n",
            "Epoch 22/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 857ms/step - accuracy: 1.0000 - loss: 0.0102 - val_accuracy: 0.7750 - val_loss: 0.8412\n",
            "Epoch 23/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 785ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.7750 - val_loss: 0.8480\n",
            "Epoch 24/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 793ms/step - accuracy: 1.0000 - loss: 0.0068 - val_accuracy: 0.7750 - val_loss: 0.8721\n",
            "Epoch 25/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 802ms/step - accuracy: 1.0000 - loss: 0.0061 - val_accuracy: 0.7700 - val_loss: 0.8676\n",
            "Epoch 26/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 859ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.7750 - val_loss: 0.8734\n",
            "Epoch 27/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 789ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.7800 - val_loss: 0.8724\n",
            "Epoch 28/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 934ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.7750 - val_loss: 0.8697\n",
            "Epoch 29/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 788ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.7800 - val_loss: 0.8972\n",
            "Epoch 30/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 880ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.7800 - val_loss: 0.8842\n",
            "Epoch 31/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 791ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.7900 - val_loss: 0.8779\n",
            "Epoch 32/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 861ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7550 - val_loss: 0.9179\n",
            "Epoch 33/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 795ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.7650 - val_loss: 0.8918\n",
            "Epoch 34/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 861ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.7800 - val_loss: 0.8886\n",
            "Epoch 35/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 794ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7650 - val_loss: 0.9173\n",
            "Epoch 36/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 867ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.7800 - val_loss: 0.9099\n",
            "Epoch 37/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 797ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7850 - val_loss: 0.9203\n",
            "Epoch 38/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 887ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7800 - val_loss: 0.9306\n",
            "Epoch 39/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 789ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7750 - val_loss: 0.9120\n",
            "Epoch 40/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 880ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7750 - val_loss: 0.9276\n",
            "Epoch 41/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 794ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7750 - val_loss: 0.9252\n",
            "Epoch 42/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 798ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7750 - val_loss: 0.9231\n",
            "Epoch 43/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 797ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7800 - val_loss: 0.9384\n",
            "Epoch 44/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 837ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7700 - val_loss: 0.9519\n",
            "Epoch 45/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 870ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.7750 - val_loss: 0.9452\n",
            "Epoch 46/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 955ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7800 - val_loss: 0.9505\n",
            "Epoch 47/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 805ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7700 - val_loss: 0.9532\n",
            "Epoch 48/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 802ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.7700 - val_loss: 0.9578\n",
            "Epoch 49/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 881ms/step - accuracy: 1.0000 - loss: 9.5194e-04 - val_accuracy: 0.7700 - val_loss: 0.9647\n",
            "Epoch 50/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 801ms/step - accuracy: 1.0000 - loss: 8.3911e-04 - val_accuracy: 0.7700 - val_loss: 0.9608\n",
            "Epoch 51/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 871ms/step - accuracy: 1.0000 - loss: 8.5139e-04 - val_accuracy: 0.7800 - val_loss: 0.9758\n",
            "Epoch 52/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 782ms/step - accuracy: 1.0000 - loss: 7.5419e-04 - val_accuracy: 0.7650 - val_loss: 0.9813\n",
            "Epoch 53/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 865ms/step - accuracy: 1.0000 - loss: 7.9667e-04 - val_accuracy: 0.7800 - val_loss: 0.9847\n",
            "Epoch 54/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 803ms/step - accuracy: 1.0000 - loss: 6.4371e-04 - val_accuracy: 0.7750 - val_loss: 0.9785\n",
            "Epoch 55/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 894ms/step - accuracy: 1.0000 - loss: 8.3212e-04 - val_accuracy: 0.7800 - val_loss: 0.9825\n",
            "Epoch 56/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 794ms/step - accuracy: 1.0000 - loss: 6.5247e-04 - val_accuracy: 0.7800 - val_loss: 0.9876\n",
            "Epoch 57/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 895ms/step - accuracy: 1.0000 - loss: 6.5428e-04 - val_accuracy: 0.7700 - val_loss: 0.9923\n",
            "Epoch 58/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 799ms/step - accuracy: 1.0000 - loss: 5.9779e-04 - val_accuracy: 0.7750 - val_loss: 0.9975\n",
            "Epoch 59/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 876ms/step - accuracy: 1.0000 - loss: 5.4993e-04 - val_accuracy: 0.7700 - val_loss: 0.9942\n",
            "Epoch 60/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 796ms/step - accuracy: 1.0000 - loss: 7.2486e-04 - val_accuracy: 0.7750 - val_loss: 0.9979\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 0.7799 - loss: 0.9121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.9979, Accuracy: 0.7750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar modelos"
      ],
      "metadata": {
        "id": "Dsdwy8Vj2fHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the first model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/datasets/models/model.h5')\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/datasets/Data/genres_original'\n",
        "IMG_PATH = '/content/drive/MyDrive/datasets/Data/images_original'\n",
        "FEATURES_PATH = '/content/drive/MyDrive/datasets/Data/features_30_sec.csv'\n",
        "\n",
        "data_features, labels = process_csv(FEATURES_PATH)\n",
        "img_features = process_img_data(IMG_PATH)\n",
        "\n",
        "features_standardized = StandardScaler().fit_transform(data_features)\n",
        "\n",
        "X_train_features, X_val_features, X_train_images, X_val_images, y_train, y_val = train_test_split(\n",
        "    features_standardized, img_features, labels, test_size=0.2, random_state=17\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate([X_val_features, X_val_images], y_val)\n",
        "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMRlcV0huAKF",
        "outputId": "a9c50432-cf71-4bab-8977-d76bc3508f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data features processing completed.\n",
            "\n",
            "Images processing completed.\n",
            "\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 185ms/step - accuracy: 0.6873 - loss: 1.6605\n",
            "\n",
            "Accuracy: 68.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the second model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/datasets/models/modelv2.h5')\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/datasets/Data/genres_original'\n",
        "IMG_PATH = '/content/drive/MyDrive/datasets/mel_img'\n",
        "FEATURES_PATH = '/content/drive/MyDrive/datasets/Data/features_30_sec.csv'\n",
        "\n",
        "data_features, labels = process_csv(FEATURES_PATH)\n",
        "img_features = process_img_data(IMG_PATH)\n",
        "\n",
        "features_standardized = StandardScaler().fit_transform(data_features)\n",
        "\n",
        "X_train_features, X_val_features, X_train_images, X_val_images, y_train, y_val = train_test_split(\n",
        "    features_standardized, img_features, labels, test_size=0.2, random_state=17\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate([X_val_features, X_val_images], y_val)\n",
        "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jMV4O_sCAde",
        "outputId": "2cae67d7-2862-4901-da1b-a638e88fd04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data features processing completed.\n",
            "\n",
            "Images processing completed.\n",
            "\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 192ms/step - accuracy: 0.7876 - loss: 1.0404\n",
            "\n",
            "Accuracy: 79.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Código extra"
      ],
      "metadata": {
        "id": "QNwB6ItT2qJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_audio_features(audio_path):\n",
        "  y, sr = librosa.load(audio_path)\n",
        "\n",
        "  features = {}\n",
        "\n",
        "  features['chroma_stft_mean'] = np.mean(librosa.feature.chroma_stft(y=y, sr=sr))\n",
        "  features['chroma_stft_var'] = np.var(librosa.feature.chroma_stft(y=y, sr=sr))\n",
        "  features['rms_mean'] = np.mean(librosa.feature.rms(y=y))\n",
        "  features['rms_var'] = np.var(librosa.feature.rms(y=y))\n",
        "  features['spectral_centroid_mean'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "  features['spectral_centroid_var'] = np.var(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "  features['spectral_bandwidth_mean'] = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
        "  features['spectral_bandwidth_var'] = np.var(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
        "  features['rolloff_mean'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
        "  features['rolloff_var'] = np.var(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
        "  features['zero_crossing_rate_mean'] = np.mean(librosa.feature.zero_crossing_rate(y=y))\n",
        "  features['zero_crossing_rate_var'] = np.var(librosa.feature.zero_crossing_rate(y=y))\n",
        "  features['harmony_mean'] = np.mean(librosa.effects.harmonic(y))\n",
        "  features['harmony_var'] = np.var(librosa.effects.harmonic(y))\n",
        "  features['perceptr_mean'] = np.mean(librosa.effects.percussive(y))\n",
        "  features['perceptr_var'] = np.var(librosa.effects.percussive(y))\n",
        "\n",
        "  tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "  features['tempo'] = tempo\n",
        "\n",
        "  mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
        "  for i in range(1, 21):\n",
        "    features[f'mfcc{i}_mean'] = np.mean(mfccs[i-1])\n",
        "    features[f'mfcc{i}_var'] = np.var(mfccs[i-1])\n",
        "\n",
        "  return pd.DataFrame(features)"
      ],
      "metadata": {
        "id": "goM5EFcEF_TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_audio(link):\n",
        "  OUTPUT_DIR = '/content/sample_data'\n",
        "\n",
        "  ydl_opts = {\n",
        "      'format': 'bestaudio/best',\n",
        "      'outtmpl': os.path.join(OUTPUT_DIR, '%(title)s.%(ext)s'),\n",
        "      'quiet': True\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "      info_dict = ydl.extract_info(link, download=True)\n",
        "      title = info_dict.get('title', None)\n",
        "\n",
        "      downloaded_file = os.path.join(OUTPUT_DIR, str(title) + '.webm')\n",
        "      if not os.path.exists(downloaded_file):\n",
        "        print(f\"Error: Downloaded file not found at {downloaded_file}\")\n",
        "        return None\n",
        "\n",
        "      audio = AudioSegment.from_file(os.path.join(OUTPUT_DIR, str(title) + '.webm'), format='webm')\n",
        "      audio.export(os.path.join(OUTPUT_DIR, str(title) + '.wav'), format='wav')\n",
        "      os.remove(os.path.join(OUTPUT_DIR, str(title) + '.webm'))\n",
        "      print('Audio downloaded')\n",
        "\n",
        "    return os.path.join(OUTPUT_DIR, str(title) + '.wav'), title\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"There was a problem downloading the video: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "lAtMy9emEtJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_clip(input_file, output_file):\n",
        "  audio = AudioFileClip(input_file)\n",
        "\n",
        "  audio_duration = audio.duration\n",
        "  start_time = (audio_duration - 30) / 2\n",
        "  end_time = start_time + 30\n",
        "\n",
        "  clip = audio.subclip(start_time, end_time)\n",
        "  clip.write_audiofile(output_file, verbose=False, logger=None)\n",
        "\n",
        "  os.remove(input_file)\n",
        "  print(f\"Clip extracted successfully\\n\")\n",
        "\n",
        "  return output_file"
      ],
      "metadata": {
        "id": "ARuPT7pcffqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mel_img(clip):\n",
        "  y, sr = librosa.load(clip)\n",
        "  mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "  mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "  img = (mel_spectrogram_db - mel_spectrogram_db.min()) / (mel_spectrogram_db.max() - mel_spectrogram_db.min())\n",
        "\n",
        "  img_grayscale = Image.fromarray((img * 255).astype(np.uint8)).convert('L')\n",
        "  img_colored = np.stack((img,) * 3, axis=-1)\n",
        "  img_colored = (img_colored * 255).astype(np.uint8)\n",
        "  image = Image.fromarray(img_colored, mode='RGB')\n",
        "\n",
        "  image.save(clip.replace('.wav', '.png'))\n",
        "\n",
        "  return clip.replace('.wav', '.png')"
      ],
      "metadata": {
        "id": "tUSNRePFj5e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_genre(clip_path, model):\n",
        "  audio_features = extract_audio_features(clip_path)\n",
        "\n",
        "  img_path = extract_mel_img(clip_path)\n",
        "  img_features = extract_img_features(img_path)\n",
        "\n",
        "  feature_scaler = joblib.load('/content/drive/MyDrive/datasets/scalers/feature_scaler.pkl')\n",
        "  features_standardized = feature_scaler.transform(audio_features)\n",
        "\n",
        "  prediction = model.predict([features_standardized, np.expand_dims(img_features, axis=0)])\n",
        "\n",
        "  genre_dict = {\n",
        "      0: 'blues',\n",
        "      1: 'classical',\n",
        "      2: 'country',\n",
        "      3: 'disco',\n",
        "      4: 'hiphop',\n",
        "      5: 'jazz',\n",
        "      6: 'metal',\n",
        "      7: 'pop',\n",
        "      8: 'reggae',\n",
        "      9: 'rock'\n",
        "  }\n",
        "\n",
        "  top_3_indices = np.argpartition(prediction[0], -3)[-3:]\n",
        "  top_3_indices = top_3_indices[np.argsort(-prediction[0][top_3_indices])]\n",
        "\n",
        "  genres = [genre_dict[i] for i in top_3_indices]\n",
        "  probs = prediction[0][top_3_indices]\n",
        "\n",
        "  predicted_genre_index = np.argmax(prediction)\n",
        "  predicted_genre = genre_dict[predicted_genre_index]\n",
        "\n",
        "  return predicted_genre, list(zip(genres, probs))"
      ],
      "metadata": {
        "id": "p8K-s-8Cn5DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/sample_data'\n",
        "\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/datasets/models/modelv2.h5')\n",
        "\n",
        "URL = input('\\nIngresa la URL de YouTube: ').strip()\n",
        "audio_path, title = download_audio(URL)\n",
        "\n",
        "audio_clip_path = extract_clip(audio_path, os.path.join(PATH, 'clip.wav'))\n",
        "\n",
        "predicted_genre, predictions = predict_genre(audio_clip_path, model)\n",
        "\n",
        "os.remove(audio_clip_path)\n",
        "os.remove(audio_clip_path.replace('.wav', '.png'))\n",
        "\n",
        "print(f'\\nVideo: {title}')\n",
        "print(f\"Género predicho: {predicted_genre}\")\n",
        "\n",
        "print(\"\\nTop 3 predicted genres:\")\n",
        "for genre, prob in predictions:\n",
        "    print(f\"{genre}: {prob:.2%}\")"
      ],
      "metadata": {
        "id": "ibakQ_calKih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}